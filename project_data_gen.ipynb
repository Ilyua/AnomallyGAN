{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b812daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import grad as torch_grad\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.utils.tensorboard import writer, SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import RandomSampler\n",
    "from math import pi\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score,recall_score,precision_score\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# from datasets.datasets import Sines\n",
    "# from models.wgangp import Generator, Critic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d332bee",
   "metadata": {},
   "source": [
    "# Articles\n",
    "+ https://arxiv.org/pdf/1803.01271.pdf -TCN\n",
    "+ https://arxiv.org/pdf/2005.01181.pdf - GAN Forecasting\n",
    "+ https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf - TimeGAN (откуда у них реконстрашн лосс?)\n",
    "+ https://habr.com/ru/post/352794/ gan explained\n",
    "+ https://habr.com/ru/post/447494/\n",
    "+ https://www.analyticsvidhya.com/blog/2017/06/introductory-generative-adversarial-networks-gans/ - обучение GAN\n",
    "+ https://id-lab.ru/posts/developers/sovety-po-obucheniyu-stabilnyh-generativno-sostyazatelnyh-setej-gan/ обучение GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1427dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "\n",
    "normal_train = pyreadr.read_r('data/TEP_FaultFree_Training.RData') # also works for Rds\n",
    "normal_test = pyreadr.read_r('data/TEP_FaultFree_Testing.RData')\n",
    "faulty_train = pyreadr.read_r('data/TEP_Faulty_Training.RData')\n",
    "faulty_test = pyreadr.read_r('data/TEP_Faulty_Testing.RData')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_tr = normal_train['fault_free_training']\n",
    "df_n_te = normal_test['fault_free_testing']\n",
    "df_a_tr = faulty_train['faulty_training']\n",
    "df_a_te = faulty_test['faulty_testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced8914",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_a_tr.groupby(['faultNumber','simulationRun'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b34414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df_n_tr,df_n_te,df_a_tr,df_a_te,seed=1,fault_number=1):\n",
    "    \n",
    "    train_normal=df_n_tr[df_n_tr['simulationRun']==seed]\n",
    "    test_normal=df_n_te[df_n_te['simulationRun']==seed]\n",
    "    \n",
    "    train_abnormal=df_a_tr[(df_a_tr['faultNumber']==fault_number)&(df_a_tr['simulationRun']==seed)]\n",
    "    test_abnormal=df_a_te[(df_a_te['faultNumber']==fault_number)&(df_a_te['simulationRun']==seed)]\n",
    "    \n",
    "    return train_normal,test_normal,train_abnormal,test_abnormal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a377f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_normal,test_normal,train_abnormal,test_abnormal = get_data(df_n_tr,df_n_te,df_a_tr,df_a_te,seed=1,fault_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08ffc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6',\n",
    "       'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12',\n",
    "       'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18',\n",
    "       'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24',\n",
    "       'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30',\n",
    "       'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36',\n",
    "       'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1',\n",
    "       'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9',\n",
    "       'xmv_10', 'xmv_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de73894",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPrettyDataset(Dataset):\n",
    "    \"\"\"The most sexy dataset in the world\"\"\"\n",
    "\n",
    "    def __init__(self, data,sample_len=15):\n",
    "        \n",
    "        \n",
    "        self.features=['xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6',\n",
    "       'xmeas_7', 'xmeas_8', 'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12',\n",
    "       'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', 'xmeas_18',\n",
    "       'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24',\n",
    "       'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30',\n",
    "       'xmeas_31', 'xmeas_32', 'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36',\n",
    "       'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', 'xmv_1',\n",
    "       'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9',\n",
    "       'xmv_10', 'xmv_11']\n",
    "        self.data = data\n",
    "        self.group_size=500\n",
    "        self.grouped = self.data.groupby(['faultNumber','simulationRun'])\n",
    "        self.sample_len=sample_len\n",
    "        self.max_group_index = self.group_size - self.sample_len +1\n",
    "        self.groups_count = len(self.grouped)\n",
    "        self.group_keys=list(self.grouped.groups.keys())\n",
    "        \n",
    "        self.groups = [self.grouped.get_group(key) for key in self.group_keys]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grouped)*(self.max_group_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        group_index = idx//(self.max_group_index)\n",
    "        idx = idx%(self.max_group_index)\n",
    "        group = self.groups[group_index]#self.grouped.get_group(self.group_keys[group_index])\n",
    "        \n",
    "        fault_numbers = group['faultNumber'].unique()\n",
    "\n",
    "        is_anomal = False\n",
    "        if len(fault_numbers)>1:\n",
    "            raise Exception\n",
    "        elif len(fault_numbers)==1:\n",
    "            fault = fault_numbers[0]\n",
    "            if fault >=1:\n",
    "                is_anomal=1\n",
    "            else:\n",
    "                is_anomal=0\n",
    "        \n",
    "        return group.iloc[idx:idx+self.sample_len][self.features].values.astype(float),is_anomal\n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a06518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conc = pd.concat([df_n_tr,df_a_tr[df_a_tr['faultNumber'].isin([1,2,3])]])\n",
    "conc = pd.concat([df_n_tr,df_a_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(conc[features].values)\n",
    "values = scaler.transform(conc[features].values)\n",
    "conc_scaled = conc.copy()\n",
    "for i, col in enumerate(features):\n",
    "    conc_scaled[col] = values[:, i]\n",
    "conc_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPrettyDataset(Dataset):\n",
    "    \"\"\"The most sexy dataset in the world\"\"\"\n",
    "\n",
    "    def __init__(self, sample_len=15):\n",
    "        \n",
    "        \n",
    "       \n",
    "        self.sample_len=sample_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10_000#len(self.grouped)*(self.max_group_index)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t = torch.tensor(0)\n",
    "        t=t.new_full((self.sample_len,52),idx%2)\n",
    "        \n",
    "        return t,idx%2\n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ad539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c110fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,noise_dim=8,target_len=450,hidden_dim=16,label_emb_dim=8,input_dim=8,dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_embedder = nn.Embedding(2,label_emb_dim)\n",
    "        self.predictor = nn.Sequential(\n",
    "                nn.Linear(hidden_dim,128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            \n",
    "                nn.Linear(128, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            \n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(),\n",
    "            \n",
    "                nn.Linear(256, 52)\n",
    "        )\n",
    "        self.label_noise_embedder = nn.Linear(label_emb_dim,52)#noise_dim,52)\n",
    "        \n",
    "        self.generator = nn.LSTM(52,hidden_dim,batch_first=True)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.target_len = target_len\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,noise,labels):\n",
    "        \n",
    "        \n",
    "        label_embedding = self.label_embedder(labels).squeeze(1)\n",
    "        \n",
    "#         input = self.activation(self.label_noise_embedder(torch.cat([label_embedding,noise],dim=1))).unsqueeze(1)\n",
    "        input = label_embedding+noise\n",
    "        input = self.label_noise_embedder(input).unsqueeze(1)\n",
    "#         print(input.shape)\n",
    "    \n",
    "        hidden=None\n",
    "        for i in range(self.target_len):\n",
    "            output,hidden = self.generator(input[:,i,:].unsqueeze(1),hidden)\n",
    "            output =self.activation(self.predictor(output[:,-1,:]))\n",
    "            # print(input.shape,output.shape)\n",
    "            input = torch.cat([input,output.unsqueeze(1)],dim=1)\n",
    "            \n",
    "        return input[:,1:,:]\n",
    "            \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self,noise_dim,target_len=450,hidden_dim=16,label_emb_dim=3,input_dim=8,dropout=0.2):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.label_embedder = nn.Embedding(2,label_emb_dim)\n",
    "#         self.predictor = nn.Sequential(\n",
    "#                 nn.Linear(hidden_dim,8),\n",
    "#                 nn.BatchNorm1d(8),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Dropout(dropout),\n",
    "            \n",
    "#                 nn.Linear(8, 16),\n",
    "#                 nn.BatchNorm1d(16),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Dropout(dropout),\n",
    "            \n",
    "#                 nn.Linear(16, 32),\n",
    "#                 nn.BatchNorm1d(32),\n",
    "#                 nn.ReLU(),\n",
    "            \n",
    "#                 nn.Linear(32, 52)\n",
    "#         )\n",
    "#         self.label_noise_embedder = nn.Linear(label_emb_dim+noise_dim,52)\n",
    "        \n",
    "#         self.generator = nn.LSTM(52,hidden_dim,batch_first=True)\n",
    "        \n",
    "#         self.activation = nn.LeakyReLU()\n",
    "#         self.target_len = target_len\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "#     def forward(self,noise,labels):\n",
    "        \n",
    "        \n",
    "#         label_embedding = self.label_embedder(labels).squeeze(1)\n",
    "        \n",
    "#         input = self.activation(self.label_noise_embedder(torch.cat([label_embedding,noise],dim=1))).unsqueeze(1)\n",
    "        \n",
    "        \n",
    "#         hidden=None\n",
    "#         for i in range(self.target_len):\n",
    "#             output,hidden = self.generator(input[:,i,:].unsqueeze(1),hidden)\n",
    "#             output =self.activation(self.predictor(output[:,-1,:]))\n",
    "#             # print(input.shape,output.shape)\n",
    "#             input = torch.cat([input,output.unsqueeze(1)],dim=1)\n",
    "            \n",
    "#         return input[:,1:,:]\n",
    "            \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88edc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,detectors_count=52,hidden_dim=8,label_emb_dim=3,dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(detectors_count,hidden_dim,batch_first=True,num_layers=2)\n",
    "        self.label_embedder  = nn.Embedding(2,label_emb_dim)\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Linear(hidden_dim+label_emb_dim, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            \n",
    "            \n",
    "                nn.Linear(512, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            \n",
    "                nn.Linear(128, 8),\n",
    "                nn.BatchNorm1d(8),\n",
    "                nn.LeakyReLU(),\n",
    "            \n",
    "                \n",
    "            \n",
    "                nn.Linear(8, 2),\n",
    "                nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,input,labels):\n",
    "        label_embedding = self.label_embedder(labels)\n",
    "        output,hidden = self.lstm(input.float())\n",
    "        x = output[:,-1,:]\n",
    "        x = torch.cat([x,label_embedding],dim=1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3471405",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM=8\n",
    "BATCH_SIZE=100\n",
    "EPOCHS=10000\n",
    "SAMPLE_LEN=10\n",
    "PART_OF_SYNTHETIC=2 # поровну \n",
    "\n",
    "anomally_label=1\n",
    "normal_label=0\n",
    "\n",
    "DISCRIMINATOR_EPOCH=10\n",
    "\n",
    "\n",
    "g = Generator(noise_dim=8,target_len=SAMPLE_LEN,hidden_dim=16,label_emb_dim=8,input_dim=8)\n",
    "d = Discriminator()\n",
    "\n",
    "\n",
    "\n",
    "g_opt = torch.optim.Adam(g.parameters(), lr = 0.001)\n",
    "\n",
    "d_opt = torch.optim.Adam(d.parameters(), lr = 0.001) # \n",
    "\n",
    "\n",
    "\n",
    "g_loss_f = nn.BCELoss()\n",
    "d_loss_f = nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyPrettyDataset(sample_len=SAMPLE_LEN)#conc_scaled,sample_len=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207acbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = Subset(dataset,np.random.randint(len(dataset),size=10**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data_subset,batch_size=BATCH_SIZE,num_workers=0,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91976c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights(dataset,weights_d={0:0.5,1:0.1}):\n",
    "    weights=[]\n",
    "    for x,y in tqdm(dataset):\n",
    "        weights.append(weights_d[y])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = make_weights(data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a52a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_gradients_in_model(model, logger, step):\n",
    "    for tag, value in model.named_parameters():\n",
    "        if value.grad is not None:\n",
    "#             print(value.grad.cpu())\n",
    "            logger.add_histogram(model._get_name()+'/'+tag + \"/grad\", value.grad.cpu(), step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6592f-f093-458b-8c71-24d0f6483362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_distribution(generator,label=0,noise_dim=8,num_samples=10000):\n",
    "    generator.eval()\n",
    "    noise = torch.randn((num_samples,noise_dim))\n",
    "    if label==0:\n",
    "        labels=torch.zeros((num_samples,1),dtype=torch.int32)\n",
    "    elif label==1:\n",
    "        labels=torch.ones((num_samples,1),dtype=torch.int32)\n",
    "    output = generator(noise,labels)\n",
    "    output = output[:,-1,0]\n",
    "    return output.flatten()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise=torch.randn((real_data.shape[0],NOISE_DIM))\n",
    "fixed_labels=anomally_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(batch_size,labels):\n",
    "    \n",
    "    noise = torch.rand(BATCH_SIZE, NOISE_DIM)\n",
    "    gen_data = g(noise,labels)\n",
    "    pred = d(gen_data,labels)\n",
    "\n",
    "    target = torch.ones(BATCH_SIZE).long()\n",
    "#     print(target.shape,pred.shape)\n",
    "    loss = F.cross_entropy(pred, target)\n",
    "    \n",
    "    \n",
    "    g_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    g_opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5086ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dics(batch,labels):\n",
    "    \n",
    "    \n",
    "    real_data = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise = torch.rand(BATCH_SIZE, NOISE_DIM)\n",
    "        gen_data = g(noise,labels)\n",
    "    \n",
    "    data = torch.cat([gen_data, real_data])\n",
    "    \n",
    "    pred = d(data,torch.cat([labels,labels]))\n",
    "\n",
    "    target = torch.cat([torch.zeros(BATCH_SIZE), \n",
    "                        torch.ones(BATCH_SIZE)]).long()\n",
    "\n",
    "    loss = F.cross_entropy(pred, target)\n",
    "#     return pred,target\n",
    "    d_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    d_opt.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "for epoch in tqdm(range(EPOCHS),desc='Epoch'):\n",
    "    d_sum_loss=0\n",
    "    g_sum_loss = 0\n",
    "    for real_data,anomally_labels in tqdm(dataloader,desc='Batch'):\n",
    "        for _ in range(5):\n",
    "            disc_loss = train_dics(real_data,anomally_labels)\n",
    "        gen_loss = train_gen(BATCH_SIZE,anomally_labels)\n",
    "#         print(disc_loss,gen_loss)\n",
    "        d_sum_loss+=disc_loss\n",
    "        g_sum_loss+=gen_loss\n",
    "        \n",
    "    # Inference generator\n",
    "    # with torch.no_grad():\n",
    "    #     g.eval()\n",
    "    #     noise = torch.randn(size=(1,NOISE_DIM))\n",
    "    #     idxs = [0]\n",
    "    #     fake_data=g(data[idxs],label=anomally_labels[idxs],noise=noise)\n",
    "    \n",
    "    # log_gradients_in_model(g, writer, epoch)\n",
    "    # log_gradients_in_model(d, writer, epoch)\n",
    "    writer.add_histogram(\"Distribution generator output\", g(noise=fixed_noise,labels=fixed_labels).flatten(), epoch)\n",
    "    \n",
    "    writer.add_histogram(\"Distribution discriminator output\", d(g(noise=fixed_noise,labels=fixed_labels),labels=fixed_labels).flatten(), epoch)\n",
    "    # writer.add_scalars('Components',\n",
    "    #                    {f'Real component':data[0,-1,0].item(),\n",
    "    #                     'Synthetic component':fake_data[0,-1,0].item()},\n",
    "    #                    epoch)\n",
    "    writer.add_scalar('Labels count',anomally_labels.sum(),epoch)\n",
    "#     writer.add_scalar('Classification metrics/ROC AUC',roc_auc_score(train_anomally_labels.numpy(),d_output.numpy(),labels=None),epoch)\n",
    "#     writer.add_scalar('Classification metrics/Recall ',recall_score(train_anomally_labels.numpy(),(d_output.numpy()>0.5)*1),epoch)\n",
    "#     writer.add_scalar('Classification metrics/Precision ',precision_score(train_anomally_labels.numpy(),(d_output.numpy()>0.5)*1,zero_division=0),epoch)\n",
    "    writer.add_scalar('Loss/generator loss',g_sum_loss/len(dataloader),epoch)\n",
    "    writer.add_scalar('Loss/discriminator loss',d_sum_loss/len(dataloader),epoch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597db2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a26a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a2f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "for epoch in tqdm(range(EPOCHS),desc='Epoch'):\n",
    "    d_sum_loss=0\n",
    "    g_sum_loss = 0\n",
    "    for real_data,anomally_labels in tqdm(dataloader,desc='Batch'):\n",
    "        d.train()\n",
    "        g.train()\n",
    "        \n",
    "        # Optimize discriminator\n",
    "        noise=torch.randn((real_data.shape[0],NOISE_DIM))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_data=g(noise=noise,labels=anomally_labels)\n",
    "        \n",
    "        \n",
    "        data = torch.cat([real_data,fake_data],dim=0)\n",
    "        train_anomally_labels = torch.cat([anomally_labels,anomally_labels],dim=0)\n",
    "        \n",
    "        is_generated_labels = torch.cat([torch.zeros(real_data.shape[0]),torch.ones(fake_data.shape[0])],dim=0).unsqueeze(1)\n",
    "        inv_is_generated_labels = (is_generated_labels+1)%2\n",
    "        for _ in range(DISCRIMINATOR_EPOCH):\n",
    "        \n",
    "            d_output = d(data,labels=train_anomally_labels)\n",
    "\n",
    "            d_loss = d_loss_f(d_output,is_generated_labels) \n",
    "            d_sum_loss+=d_loss.item()\n",
    "            d_opt.zero_grad()\n",
    "            d_loss.backward()\n",
    "            d_opt.step()\n",
    "        \n",
    "        \n",
    "      \n",
    "        # Optimize generator\n",
    "        fake_data=g(labels=anomally_labels,noise=noise)\n",
    "        # print(fake_data)\n",
    "        \n",
    "        \n",
    "        data = torch.cat([real_data,fake_data],dim=0)\n",
    "        train_anomally_labels = torch.cat([anomally_labels,anomally_labels],dim=0)\n",
    "        \n",
    "        is_generated_labels = torch.cat([torch.zeros(real_data.shape[0]),torch.ones(fake_data.shape[0])],dim=0).unsqueeze(1)\n",
    "        inv_is_generated_labels = (is_generated_labels+1)%2\n",
    "        \n",
    "        \n",
    "        d_output = d(data,labels=train_anomally_labels)\n",
    "        \n",
    "        g_loss = g_loss_f(d_output,inv_is_generated_labels)\n",
    "        g_sum_loss+=g_loss.item()\n",
    "        \n",
    "        g_opt.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_opt.step()\n",
    "        fake_data=g(labels=anomally_labels,noise=noise)\n",
    "#         print(fake_data)\n",
    "        \n",
    "        \n",
    "    # Inference discriminator\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        d.eval()\n",
    "        d_output = d(data,labels=torch.zeros(data.shape[0],dtype=torch.int))\n",
    "        \n",
    "        \n",
    "    # Inference generator\n",
    "    # with torch.no_grad():\n",
    "    #     g.eval()\n",
    "    #     noise = torch.randn(size=(1,NOISE_DIM))\n",
    "    #     idxs = [0]\n",
    "    #     fake_data=g(data[idxs],label=anomally_labels[idxs],noise=noise)\n",
    "    \n",
    "    # log_gradients_in_model(g, writer, epoch)\n",
    "    # log_gradients_in_model(d, writer, epoch)\n",
    "    writer.add_histogram(\"Distribution generator output\", g(noise=fixed_noise,labels=fixed_labels).flatten(), epoch)\n",
    "    # writer.add_scalars('Components',\n",
    "    #                    {f'Real component':data[0,-1,0].item(),\n",
    "    #                     'Synthetic component':fake_data[0,-1,0].item()},\n",
    "    #                    epoch)\n",
    "    writer.add_scalar('Labels count',anomally_labels.sum(),epoch)\n",
    "    writer.add_scalar('Classification metrics/ROC AUC',roc_auc_score(train_anomally_labels.numpy(),d_output.numpy(),labels=None),epoch)\n",
    "    writer.add_scalar('Classification metrics/Recall ',recall_score(train_anomally_labels.numpy(),(d_output.numpy()>0.5)*1),epoch)\n",
    "    writer.add_scalar('Classification metrics/Precision ',precision_score(train_anomally_labels.numpy(),(d_output.numpy()>0.5)*1,zero_division=0),epoch)\n",
    "    writer.add_scalar('Loss/generator loss',g_sum_loss/len(dataloader),epoch)\n",
    "    writer.add_scalar('Loss/discriminator loss',d_sum_loss/len(dataloader),epoch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47ead4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
